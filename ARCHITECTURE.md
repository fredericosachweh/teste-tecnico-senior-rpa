# ğŸ—ï¸ Arquitetura do Sistema

## VisÃ£o Geral

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI   â”‚â”€â”€â”€â”€â–¶â”‚  RabbitMQ   â”‚â”€â”€â”€â”€â–¶â”‚   Workers   â”‚
â”‚    (API)    â”‚     â”‚   (Queue)   â”‚     â”‚  (Crawlers) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                       â”‚
       â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  PostgreSQL â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚    (Data)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Componentes

### 1. FastAPI (API REST)
**Arquivo:** `app/main.py`

**Responsabilidades:**
- Receber requisiÃ§Ãµes HTTP dos clientes
- Criar jobs no banco de dados
- Publicar mensagens na fila RabbitMQ
- Retornar status e resultados dos jobs

**Endpoints:**
- `POST /crawl/hockey` - Agenda coleta de Hockey
- `POST /crawl/oscar` - Agenda coleta de Oscar
- `POST /crawl/all` - Agenda ambas coletas
- `GET /jobs` - Lista todos os jobs
- `GET /jobs/{job_id}` - Status de um job especÃ­fico
- `GET /jobs/{job_id}/results` - Resultados de um job
- `GET /results/hockey` - Todos os dados de Hockey
- `GET /results/oscar` - Todos os dados de Oscar

### 2. RabbitMQ (Fila de Mensagens)
**Arquivo:** `app/queue.py`

**Responsabilidades:**
- Gerenciar fila de jobs assÃ­ncronos
- Garantir entrega das mensagens
- Balancear carga entre workers
- Persistir mensagens (durable queue)

**ConfiguraÃ§Ã£o:**
- Queue Name: `crawl_jobs`
- Durable: `True`
- QoS: `prefetch_count=1` (um job por worker)
- Socket timeout e `connection_attempts` para falha rÃ¡pida se RabbitMQ estiver indisponÃ­vel
- `publish_jobs()` para publicar vÃ¡rios jobs em uma Ãºnica conexÃ£o (ex.: `/crawl/all`)

### 3. Workers (Processadores)
**Arquivo:** `app/worker.py`

**Responsabilidades:**
- Consumir mensagens da fila RabbitMQ
- Executar os scrapers (Hockey ou Oscar)
- Atualizar status dos jobs no banco
- Salvar resultados no banco de dados

**Fluxo:**
1. Recebe mensagem com `{job_id, job_type}`
2. Atualiza job para status `running`
3. Executa scraper apropriado
4. Salva resultados no banco
5. Atualiza job para `completed` ou `failed`
6. Faz ACK da mensagem

**Escalabilidade:**
- Configurado com 2 rÃ©plicas por padrÃ£o
- Pode ser escalado: `docker-compose up --scale worker=4`

### 4. PostgreSQL (Banco de Dados)
**Arquivos:** `app/database.py`, `app/models/*.py` (jobs, hockey_teams, films)

**Tabelas:**

#### `jobs`
```sql
- id (PK)
- job_id (UUID, unique)
- job_type (enum: hockey, oscar)
- status (enum: pending, running, completed, failed)
- created_at
- started_at
- completed_at
- error_message
- results_count
```

#### `hockey_team`
```sql
- id (PK)
- name (UNIQUE)
```

#### `hockey_team_historic`
```sql
- id (PK)
- team_id (FK â†’ hockey_team.id)
- year
- wins, losses, losses_ot
- wins_percentage
- goals_for, goals_against
- goal_difference
- job_id (rastreabilidade)
```

#### `films`
```sql
- id (PK)
- title (UNIQUE)
```
(Oscar metadata fica em `oscar_winner_films`.)

#### `oscar_winner_films`
```sql
- id (PK)
- film_id (FK â†’ films.id, ON DELETE CASCADE)
- year
- nominations
- awards
- best_picture (boolean)
- job_id (rastreabilidade)
```

### 5. Scrapers (Coletores)
**Arquivo:** `app/crawlers/crawler.py`

#### HockeyHistoricScraper
**Site:** https://www.scrapethissite.com/pages/forms/

**EstratÃ©gia:**
- HTML tradicional com paginaÃ§Ã£o
- Selenium com Chrome headless
- Itera por todas as pÃ¡ginas (`?page_num=N`)
- Extrai dados de tabela HTML

**Dados coletados:**
- Nome do time
- Ano
- EstatÃ­sticas (vitÃ³rias, derrotas, gols, etc.)

#### OscarScraper
**Site:** https://www.scrapethissite.com/pages/ajax-javascript/

**EstratÃ©gia:**
- Dados via AJAX (requisiÃ§Ãµes HTTP por ano)
- Selenium para obter lista de anos na pÃ¡gina; `urllib` para chamadas Ã  API AJAX por ano
- Cria/Reutiliza `Film` por tÃ­tulo; grava `OscarWinnerFilm` com `film_id`

**Dados coletados:**
- TÃ­tulo do filme (tabela `films`, Ãºnico por tÃ­tulo)
- Ano, indicaÃ§Ãµes, prÃªmios, melhor filme (tabela `oscar_winner_films`)

## Fluxo de Dados

### Fluxo de CriaÃ§Ã£o de Job

```
1. Cliente faz POST /crawl/hockey
           â†“
2. API cria Job (status=pending) no PostgreSQL
           â†“
3. API publica mensagem {job_id, job_type} no RabbitMQ
           â†“
4. API retorna job_id para o cliente imediatamente
```

### Fluxo de Processamento

```
1. Worker consome mensagem da fila
           â†“
2. Worker atualiza Job (status=running) no PostgreSQL
           â†“
3. Worker executa scraper apropriado
           â†“
4. Scraper coleta dados do site
           â†“
5. Scraper salva dados no PostgreSQL com job_id
           â†“
6. Worker atualiza Job (status=completed, results_count)
           â†“
7. Worker faz ACK da mensagem no RabbitMQ
```

### Fluxo de Consulta

```
1. Cliente faz GET /jobs/{job_id}
           â†“
2. API consulta PostgreSQL
           â†“
3. API retorna status do job
```

```
1. Cliente faz GET /jobs/{job_id}/results
           â†“
2. API verifica se job estÃ¡ completed
           â†“
3. API consulta dados relacionados ao job_id
           â†“
4. API retorna resultados
```

## PadrÃµes e Boas PrÃ¡ticas

### 1. Processamento AssÃ­ncrono
- Jobs sÃ£o processados em background
- API responde imediatamente
- Cliente pode consultar status depois

### 2. Rastreabilidade
- Cada dado coletado tem `job_id`
- PossÃ­vel saber qual job coletou cada dado
- HistÃ³rico completo de execuÃ§Ãµes

### 3. Tratamento de Erros
- Erros sÃ£o capturados e salvos em `error_message`
- Jobs com erro ficam com status `failed`
- Mensagens com erro podem ser reprocessadas

### 4. IdempotÃªncia
- DeclaraÃ§Ãµes de queue sÃ£o idempotentes
- CriaÃ§Ã£o de tabelas Ã© idempotente
- Workers podem ser reiniciados sem problemas

### 5. Escalabilidade Horizontal
- Workers podem ser escalados independentemente
- RabbitMQ distribui jobs entre workers
- Banco suporta conexÃµes concorrentes

### 6. ContainerizaÃ§Ã£o
- Cada componente em container separado
- Isolamento de dependÃªncias
- FÃ¡cil deploy e replicaÃ§Ã£o

## Tecnologias

| Componente | Tecnologia | VersÃ£o |
|------------|------------|--------|
| Runtime | Python | **3.12** |
| API | FastAPI | 0.115+ |
| ORM | SQLAlchemy | 2.0+ |
| ValidaÃ§Ã£o | Pydantic | 2.0+ |
| Banco | PostgreSQL | 16 |
| Fila | RabbitMQ | 3 |
| Scraping | Selenium | 4.24+ |
| Browser | Chrome | Stable |
| Container | Docker | Latest |
| OrquestraÃ§Ã£o | Docker Compose | Latest |

## Scripts e ferramentas

| Script | Uso |
|--------|-----|
| `python -m app.init_db` | Garante que o banco existe e cria/atualiza tabelas |

## Desenvolvimento e qualidade

- **Testes:** `pytest` em `app/tests` e `app/crawlers` (modelos, API, crawlers).
- **Lint/format:** Ruff e Black (config em `pyproject.toml`).
- **Pre-commit:** `.pre-commit-config.yaml` â€” hooks para trailing whitespace, end-of-file, YAML/JSON, Black, Ruff e Pytest. InstalaÃ§Ã£o: `pre-commit install`.

## Melhorias Futuras

1. **Cache:** Redis para cache de resultados
2. **Retry:** PolÃ­tica de retry para jobs falhados
3. **Dead Letter Queue:** Fila separada para erros
4. **MÃ©tricas:** Prometheus + Grafana
5. **Rate Limiting:** Limitar chamadas aos sites
6. **Webhook:** Notificar conclusÃ£o de jobs
7. **Agendamento:** Celery Beat para jobs periÃ³dicos
8. **API Keys:** AutenticaÃ§Ã£o na API
